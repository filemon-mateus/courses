\documentclass[10pt, letterpaper]{article}
\usepackage{preamble}

\begin{document}
\header{CS $6150$}{Filemon Mateus}{Homework \#\ $2$}{Divide \& Conquer}{\today}

\begin{enumerate}[label={\bfseries Q\arabic*.}]
  \item
    \begin{enumerate}
      \item
        {\itshape Algorithm Description} \\ \vspace{-4mm}

        We proceed with a ``Divide and Conquer'' strategy and divide the problem of merging $k$ sorted lists
        $L_1,\dots,L_k$ as follows.

        \begin{itemize}
          \item Merge $\floor{k/2}$ sorted lists $L_1,\dots,L_{\floor{k/2}}$ into one single sorted list $A_1$.
          \item Merge $\ceil{k/2}$ sorted lists $L_{\floor{k/2} + 1},\dots,L_k$ into one single sorted list $A_2$.
        \end{itemize}

        Then we conquer the two aforementioned problems with recursion and combine their respective solutions
        $A_1$ and $A_2$ into $A_3$ using the canonical \textsc{Merge} subroutine discussed in class.

      \item
        {\itshape Algorithm Pseudocode}
        \vspace{-5mm}
        \begin{center}
          \begin{minipage}{\linewidth}
            \begin{algorithm}[H]
              \caption{\textsc{Merge-Lists}$(L_1, \ldots, L_k)$}\label{alg:merge-lists}
              \begin{algorithmic}[1]
                \If{$k = 1$} \Comment{base case}
                  \State \Return $L_1$
                \EndIf
                \State $A_1 \gets \textsc{Merge-Lists}(L_1, \ldots, L_{\lfloor k/2 \rfloor})$ \Comment{conquer left subproblem with recursion}
                \State $A_2 \gets \textsc{Merge-Lists}(L_{\lfloor k/2 \rfloor + 1}, \ldots, L_k)$ \Comment{conquer right subproblem with recursion}
                \State $A_3 \gets \textsc{Merge}(A_1, A_2)$ \Comment{combine solutions with \textsc{Merge} subroutine}
                \State \Return $A_3$ 
              \end{algorithmic}
            \end{algorithm}
          \end{minipage}
        \end{center}

      \item
        {\itshape Algorithm Correctness} \\ \vspace{-4mm}

        To show the correctness of \autoref{alg:merge-lists}, we will use (strong) induction on $k$. Let $k$
        be an arbitrary integer greater or equal to $1$. Let $L_1, \ldots, L_k$ be $k$ arbitrary lists
        (with the assumption that all numbers in the lists are distinct). We wish to show that \textsc{Merge-Lists},
        on input $L_1, \ldots, L_k$, merges them into one single sorted list.

        \begin{itemize}[itemsep=10pt]
          \item
            {\itshape Base Case}: for the base case we have $k = 1$. In this case, $L_1$ is already sorted and
            Step $2$ of \autoref{alg:merge-lists} simply returns the single list $L_1$.

          \item
            {\itshape Inductive Hypothesis}: assume \autoref{alg:merge-lists} correctly merges $m$ sorted lists,
            for every $m < k$, into one single sorted list.

          \item
            {\itshape Inductive Step}: from the inductive hypothesis, it follows that $A_1$ and $A_2$ are sorted
            lists because they merge $\floor{k/2} < k$ and $\ceil{k/2} < k$ sorted lists, respectively. Since the
            \textsc{Merge} subroutine correctly merges two sorted lists into a single sorted list, $A_1$ and $A_2$
            are correctly combined into a single sorted list. We conclude then that \autoref{alg:merge-lists}
            correctly merges the $k$ sorted lists into a single sorted list. \hfill $\square$
        \end{itemize}

      \item
        {\itshape Algorithm Analysis} \\ \vspace{-4mm}

        Let $T(n, k)$ be the total running time of merging $k$ sorted lists with a total of $n$ elements. We have
        $T(n, k) = O(n)$ for $k = 1$; note $L_1$ in Step $2$ of \autoref{alg:merge-lists} will contain exactly $n$
        elements requiring potential copies so it is prudent to assume it takes time proportional to $n$ at the base
        case. For the recursion, $A_1$ is a list of size $n_1$ and $A_2$ is a list of size $n_2$ where $n_1 + n_2 =
        n$. So merging them takes $O(n)$ time. Thus, the recurrence becomes:

        \[
          T(n, k) = \begin{cases}n & \text{if}\ k = 1 \\ T(n_1, \floor{k/2}) + T(n_2, \ceil{k/2}) + n & \text{otherwise}\end{cases} \\
        \]

        To solve this recurrence, we proceed with the recursion tree approach. For simplicity assume $k$
        is a power of $2$. Then the recursion tree is a complete binary tree with $k$ nodes at the leaves and depth
        $\log k$. The work at the root node is $n$.  At the next level is $n_1 + n_2$ which is $n$. Each level
        incurs an $O(n)$ cost and there are $\log k$ levels hence the total work is $O(n \log k)$. \hfill $\square$
    \end{enumerate}

  \item
    \begin{enumerate}
      \item
        $(4,2), (4,1), (2,1), (9,1), (9,7)$.

      \item
        The array with entries from the set $\{1, 2, \ldots, n\}$ with the most inversions will have their
        entries in ``backward,'' reversed sorted order, i.e. $\{n, n-1, \ldots, 1\}$. This is because every
        single pair of array indices is inverted, so we get $n-1$ inversions with the first entry, $n-2$
        inversions with the second entry, $n-3$ inversions with the third entry, and so on. In general, for
        a fixed index $i$ we will have $n-i$ inversions. So the total number of inversions over all possible
        choices of $i$ will be:

        \[
          \dsum_{i=1}^{n} (n-i) = (n-1) + (n-2) + \cdots + 1 + 0 = \dfrac{n(n-1)}{2}
        \]

      \item
        \begin{enumerate}[label={\arabic*.}, leftmargin=*]
          \item
            {\itshape Algorithm Description} \\ \vspace{-3mm}

            We proceed with a ``Divide and Conquer'' strategy motivated by \textsc{Merge-Sort}. Specifically,
            for pairs of array indices $(i, j)$ with $i < j$, we distinguish three inversion cases: $(1)$ a
            $left$-inversion if $i, j \leq \floor{n/2}$, $(2)$ a $right$-inversion if $i, j \geq \ceil{n/2}$,
            and $(3)$ a $cross$-inversion if $i \leq \floor{n/2}$ and $j \geq \ceil{n/2}$. Similar to
            \textsc{Merge-Sort}, we start by splitting the array into two (nearly equal) halves. We then recurse
            on the left and count all the inversions that are restricted to the first half of the array. These
            are precisely the $left$-inversions. Then we invoke a second recursive call on the right to count
            all the $right$-inversions. Finally, we delegate the residual work left over (i.e. counting the
            number of $cross$-inversions) to a modified version of the \textsc{Merge} subroutine (see
            \autoref{alg:count-inversions} for details).  Since every inversion is either $left$ or $right$ or
            $cross$ and cannot be any more than one of these three, we can simply report the sum of the results
            of the two recursive calls plus the output from the aforementioned, modified \textsc{Merge} subroutine.

          \item
            {\itshape Algorithm Pseudocode}
            \vspace{-3mm}
            \begin{center}
              \begin{minipage}{\linewidth}
                \begin{algorithm}[H]
                  \caption{Reports the total number of inversions in $A[p \cdots q]$}\label{alg:count-inversions}
                  \begin{algorithmic}[1]
                    \Procedure{Count-Inversions}{$A,p,q$}
                      \If{$p \geq q$}
                        \State \Return $0$
                      \EndIf
                      \State $m \gets \lfloor(p + q) / 2\rfloor$
                      \State $l \gets \textsc{Count-Inversions}(A,p,m)$ \Comment{count $left$-inversions}
                      \State $r \gets \textsc{Count-Inversions}(A,m+1,q)$ \Comment{count $right$-inversions}
                      \State $c \gets \textsc{Cross-Inversions}(A,p,m,q)$ \Comment{count $cross$-inversions}
                      \State \Return $l + c + r$
                    \EndProcedure
                    \State
                    \Procedure{Cross-Inversions}{$A,p,m,q$} \Comment{modified \textsc{Merge} subroutine}
                      \State $n_1 \gets m - p + 1$
                      \State $n_2 \gets q - m$
                      \State $L[1 \cdots n_1] \gets A[p \cdots m]$
                      \State $R[1 \cdots n_2] \gets A[m + 1 \cdots q]$
                      \State $i \gets 1$
                      \State $j \gets 1$
                      \State $c \gets 0$ \Comment{counts \#\ of $cross$-inversions}
                      \For{$k \gets p$ \textbf{to}\ $q$}
                        \If{$n_1 < i$} \Comment{if we exhaust $L$ then copy unexhausted items of $R$ into $A$}
                          \State $A[k] \gets R[j]$
                          \State $j \gets j + 1$
                        \ElsIf{$n_2 < j$} \Comment{if we exhaust $R$ then copy unexhausted items of $L$ into $A$}
                          \State $A[k] \gets L[i]$
                          \State $i \gets i + 1$
                        \ElsIf{$L[i] \leq R[j]$} \Comment{this does not cause an inversion}
                          \State $A[k] \gets L[i]$
                          \State $i \gets i + 1$
                        \Else \Comment{but this does!}
                          \State $A[k] \gets R[j]$
                          \State $j \gets j + 1$
                          \State $c \gets c + (n_1 - i + 1)$ \Comment{inversion pairs $(L[i], R[j]), (L[i+1], R[j]),\ldots, (L[n_1], R[j])$}
                        \EndIf
                      \EndFor
                      \State \Return $c$
                    \EndProcedure
                  \end{algorithmic}
                \end{algorithm}
              \end{minipage}
            \end{center}

          \item
            {\itshape Algorithm Correctness} \\ \vspace{-3mm}

            We use (strong) induction on $n$, the number of elements in $A$, to demonstrate the correctness of
            \autoref{alg:count-inversions}. Recall the number of elements in $\{A[p], A[p+1], \ldots, A[q]\}$
            is $q - p + 1$, and henceforth, we will denote this by $|A|$. \\

            \begin{itemize}[itemsep=10pt]
              \item
                {\itshape Base Case}: when $|A| \leq 1$, notice the number of inversions in $A$ is $0$. If $A$
                contains at most one element then ($p \geq q$) and Step $3$ of \autoref{alg:count-inversions}
                gets executed---reporting $0$ as its output. It follows then that \autoref{alg:count-inversions}
                executes correctly as intended and produces the desired, expected output when $|A| \leq 1$.

              \item
                {\itshape Induction Hypothesis}: assume that \autoref{alg:count-inversions} reports the correct
                number of inversions when $|A| \leq k$, for every $k = 2, 3, \ldots, n-1$, and then sorts $A$
                from left to right.

              \item
                {\itshape Induction Step}:
                now consider when $|A|$ equals to $n$. Step $5$ of \autoref{alg:count-inversions} divides $A$ into two
                subarrays of almost equal size. Let $L$ represent the subarray $A[p \cdots m]$ and $R$ represent the
                subarray $A[m+1 \cdots q]$. Clearly $|L| < n$ and $|R| < n$. Thus, applying the inductive hypothesis,
                we are guaranteed that the number of inversions produced in Steps $6$ and $7$ are correct, i.e. $l$
                stores the correct number of inversions in $L$ and $r$ stores the correct number of inversions in $R$.
                Moreover, based on the induction hypothesis, $L$ and $R$ are now sorted so there are no longer any
                inversions in $L$ nor in $R$, only inversions between $L$ and $R$.  So let's proceed to analyze the
                \textsc{Cross-Inversions} subroutine, which we delegated earlier to handle precisely the counting of the $cross$-inversions
                between $L$ and $R$. Recall that if an element $R_j$ from the list $R$ is selected ahead of $m$ items
                from list $L$, then $R_j$ is less than the remaining $m$ elements from list $L$, corresponding to $m$
                inversions. Additionally, selecting $R_j$ to be placed as the next element in the new combined, sorted
                list eliminates all $m$ inversions. So no new inversions are created in the merging step of the
                \textsc{Cross-Inversions} subroutine. Consequently, no inversions are counted twice, since removing
                an invertion merges $L$ and $R$ into one combined list, and inversions can only exist between lists.
                Therefore, \textsc{Cross-Inversions}, along with the two recursive calls, correctly account for all
                inversions in $A$. \hfill $\square$
            \end{itemize}

          \item
            {\itshape Algorithm Analysis} \\ \vspace{-3mm}

            Let $T(n)$ represent the total running time of \autoref{alg:count-inversions} on an input of size
            $n$.  At the base case, we have $T(n) = 1$ for $n = 1$. For the recursion, we have two subproblems
            of size $\floor{n/2}$ and $\ceil{n/2}$, and since we are piggy-backing on the \textsc{Merge}
            subroutine, we have a combine step of $O(\floor{n/2} + \ceil{n/2}) = O(n)$. Thus, the recurrence
            becomes: $T(1) = 1$, $T(n) = 2T(n/2) + n$. With $a = 2$, $b = 2$, and $f(n) = n$, this recursion
            solves to $\Theta(n \log n)$, according to case $2$ of the Master Theorem discussed in class.
        \end{enumerate}
    \end{enumerate}

  \item
    \begin{enumerate}
      \item
        $T(n) = 2T(n/2) + n^3$. Here $a = 2$, $b = 2$, $f(n) = n^3$, and $n^{\log_b a} = n^{\log_2 2} = n$.
        For $\epsilon = 2$, we have $f(n) = \Omega(n^{\log_2 2 + \epsilon})$. So case $3$ of the Master
        Theorem applies if we can show that $af(n/b) \leq cf(n)$ for some $c < 1$ and all sufficiently large
        $n$. This would mean $(1/4)n^3 \leq cn^3$. Setting $c = 1/3$ would cause this condition to be satisfied.
        Hence $T(n) = \Theta(n^3)$.

      \item
        $T(n) = 4T(n/2) + n\sqrt{n}$. Here $a = 4$, $b = 2$, $f(n) = n\sqrt{n}$, and $n^{\log_b a} = n^{\log_2 4}
        = n^2$. Since $f(n) = O(n^{\log_2 4 - \epsilon})$ for $\epsilon = 0.5$, case 1 of the Master Theorem
        applies, and the solution is $T(n) = \Theta(n^2)$.

      \item
        $T(n) = 2T(n/2) + n\log n$. To solve this recurrence, we can simply expand the recurrence as follows (ommitting
        straighforward simplifications):
        \begin{align*}
          T(n) &= 2T\left(\frac{n}{2}\right) + n\log n \\
               &= 4T\left(\frac{n}{4}\right) + n\left(\log n + \log \frac{n}{2}\right) \\
               &= 8T\left(\frac{n}{8}\right) + n\left(\log n + \log \frac{n}{2} + \log \frac{n}{4}\right) \\
               % &= 16T\left(\frac{n}{16}\right) + n\left(\log n + \log \frac{n}{2} + \log \frac{n}{4} + \log \frac{n}{8}\right) \\
               &= \hspace{24mm} \vdots \\
               &= 2^kT\left(\frac{n}{2^k}\right) + n \sum_{i=0}^{k-1} \log \frac{n}{2^i} \\
               &= 2^kT\left(\frac{n}{2^k}\right) + n \left[\sum_{i=0}^{k-1} \log n - \sum_{i=0}^{k-1} \log 2^i\right] \\
               &= 2^kT\left(\frac{n}{2^k}\right) + n \left[\sum_{i=0}^{k-1} \log n - \sum_{i=0}^{k-1} i\right] \\
               &= 2^kT\left(\frac{n}{2^k}\right) + n \left[k\log n - \frac{k(k-1)}{2}\right] \\
               &= 2^kT\left(\frac{n}{2^k}\right) + nk\left(\log n - \frac{k}{2} + \frac{1}{2}\right)
        \end{align*}
        The expanding stops once $n/2^k$ is equal to $1$, which means $k = \log n$. When $k = \log n$, we have:
        \begin{align*}
          T(n) &= 2^{\log n}\ T\left(\frac{n}{2^{\log n}}\right) + n\log n\left(\log n - \frac{1}{2}\log n + \frac{1}{2}\right) \\
               &= nT(1) + n\log n \left(\frac{1}{2}\log n + \frac{1}{2}\right) \\
               &= n + \frac{1}{2}n\log^2 n  + \frac{1}{2} n\log n \\
               &= O(n\log^2 n)
        \end{align*}
        Therefore, we conclude $T(n) = O(n\log^2 n)$.

      \item
        $T(n) = T(3n/4) + n$. We guess the solution is $T(n) = O(n)$. According to the definition of big-$O$
        notation, we want to find a constant $c > 0$ and an integer $n_0 \geq 1$ such that $T(n) \leq cn$ for all
        sufficiently large $n \geq n_0$. We proceed with (strong) induction on $n$ and assume $T(n) \leq cn$ is true for all
        positive numbers less than $n$. Then, $T(3n/4) \leq (3/4)cn$ and $T(n) \leq (3/4)cn + n$. Hence, in order
        to prove $T(n) \leq cn$, it is sufficient to prove that $(3/4)cn + n \leq cn$, or equivalently $(3/4)c + 1
        \leq c$. It is easy to see that any $c \geq 4$ suffices. So for any choices of $c \geq 4$ and $n_0 = 1$,
        $T(n) \leq cn$ holds for all sufficiently large $n \geq n_0$. Therefore, we conclude $T(n) = O(n)$.
    \end{enumerate}

  \item
    \begin{enumerate}
      \item
        {\itshape Algorithm Description} \\ \vspace{-3mm}

        We proceed with a "Divide and Conquer" strategy and split the daily stock prices into two (nearly equal)
        halves. In doing so, we distinguish three possible cases for the optimal buy and sell times: $(1)$ optimal
        buy and sell times lie exclusively in the first half of the split; $(2)$ optimal buy and sell times lie
        exclusively in the second half of the split; and $(3)$ optimal buy time lies in the first half of the split
        and optimal sell time lies in the second half of the split. \\

        Similar to the inversion problem in \textbf{Q2}, we can obtain the corresponding values of $(1)$ and $(2)$
        by simply recursing on the left and right halves of the array. However, for $(3)$ we should strategize buying
        on the lowest possible day on the left and selling on the highest possible day on the right. Our tentative
        solution then becomes as follows: if the array has size $0$ or $1$, the maximum profit is $0$; otherwise, we
        split the array in half and compute the maximum profit on the left, and call it $L_{profit}$. We compute the
        maximum profit on the right, and call it $R_{profit}$. We find the minimum on the left, and call it $L_{min}$.
        We find the maximum on the right, call it $R_{max}$. We report the maximum of $L_{profit}, R_{profit}$, and
        $R_{max} - L_{min}$ as our final output. \\

        In analyzing the time complexity of our solution, it is evident that our recurrence has a base case that
        incurs an $O(1)$ cost. However, for its recursive step, we have two recursive calls on a subproblem half
        as large as the original input followed by an additional linear cost incurred by the task of finding the
        maximum and minium values across the left and right. Thus, the recurrence becomes: $T(1) = 1$, $T(n) =
        2T(n/2) + n$, which solves to $\Theta(n \log n)$---which is not quite what we want. \\

        The key idea to improve this is to delegate the linear task of finding the maximum and minimum values to
        the recursive calls. This solution works just as before, but now it never incurs an $O(n)$ cost at each
        step to find the minimum and maximum values across the left and right. In fact, it only incurs $O(1)$ work
        at each level. So we get a new recurrence of the form: $T(1) = 1$, $T(n) = 2T(n/2) + 1$, which solves
        to $O(n)$, as desired. The pseudocode for this is given below.\footnote{
          Note that this solution returns the maximum profit, not the indices/days it occurs. To get the indices/days,
          we resort to the classic ``two-sum'' problem, which can conviniently be solved in $O(n)$ time with auxiliary
          linear space. Consult \autoref{alg:max-single-sell-profit} for details.
          {\itshape Description} outline weakly adapted from: \href{https://stackoverflow.com/questions/7086464/maximum-single-sell-profit}{\url{https://stackoverflow.com/questions/7086464/maximum-profit}}.
        }

      \item
        {\itshape Algorithm Pseudocode}
        \vspace{-5mm}
        \begin{center}
          \begin{minipage}{\linewidth}
            \begin{algorithm}[H]
              \caption{Maximum Single Sell Profit}\label{alg:max-single-sell-profit}
              \begin{algorithmic}[1]
                \Procedure{Max-Single-Sell-Profit}{$A, p, q$} \Comment{maximum single sell profit from $A[p \cdots q]$}
                  \If{$p = q$}
                    \State \Return{$(0, A[p], A[q])$}
                  \EndIf
                  \State $m \gets \floor{(p + q) / 2}$
                  \State $(L_{profit}, L_{min}, L_{max}) \gets \textsc{Max-Single-Sell-Profit}(A, p, m)$
                  \State $(R_{profit}, R_{min}, R_{max}) \gets \textsc{Max-Single-Sell-Profit}(A, m + 1, q)$
                  \State $x \gets \textsc{Max}(L_{profit}, R_{profit}, R_{max} - L_{min})$
                  \State $y \gets \textsc{Min}(L_{min}, R_{min})$
                  \State $z \gets \textsc{Max}(L_{max}, R_{max})$
                  \State \Return{$(x, y, z)$}
                \EndProcedure
                \State
                \Procedure{Query-Max-Profit-Indices}{$A, x$} \Comment{classic `two-sum' problem}
                  \State $i \gets 1$
                  \State $j \gets 1$
                  \State $\mathcal{H} \gets \{\}$ \Comment{a constant time lookup table}
                  \For{$k \gets n$ \textbf{down to}\ $1$}
                    \State $y \gets x + A[k]$
                    \If{$y$ \textbf{in}\ $\mathcal{H}$}
                      \State $i \gets k$
                      \State $j \gets \mathcal{H}[y]$
                      \State \Return{$\left(i, j\right)$} \Comment{buy on day $i$, sell on day $j$}
                    \EndIf
                    \State $\mathcal{H}\left[A[k]\right] \gets k$
                  \EndFor
                  \State \Return{$\left(i, j\right)$} \Comment{here $i = j$ so no way to make money}
                \EndProcedure
              \end{algorithmic}
            \end{algorithm}
          \end{minipage}
        \end{center}

      \item
        {\itshape Algorithm Correctness} \\ \vspace{-3mm}

        We prove the correctness of \autoref{alg:max-single-sell-profit} by using (strong) induction on
        $n$---the number of permissible days to buy and sell stocks. Note the number of days in $\{A[p],
        A[p+1], \ldots, A[q]\}$ is $q - p  + 1$ and we denote by $|A|$. \\ \vspace{-2mm}

        \begin{itemize}[itemsep=10pt]
          \item
            {\itshape Base Case}: when $|A| = 1$ the maximum profit is $0$ and $q - p + 1 = 1$. So $p =
            q$ and Step $3$ of \autoref{alg:max-single-sell-profit} gets executed, reporting $0$ in its
            first entry. It follows that \autoref{alg:max-single-sell-profit} works correctly and produces
            the desired output when $|A| = 1$.

          \item
            {\itshape Induction Hypothesis}: assume that \autoref{alg:max-single-sell-profit} reports the
            correct maximum profit along with the minimum and maximum values on the left and right when
            $|A| \leq k$, for every $k = 2, 3, \ldots, n-1$.

          \item
            {\itshape Induction Step}: then it follows from the induction hypothesis that Step $6$ and $7$
            of \autoref{alg:max-single-sell-profit} report the correct maximum profit on the left and right
            ($L_{profit}, R_{profit}$), as well as the corresponding minimum and maximum values within them
            ($L_{min}, L_{max}, R_{min}$, and $R_{max}$). (This is true because on the left
            $|A| = \floor{n/2} < n$ and on the right $|A| = \ceil{n/2} < n$, and the induction hypothesis
            (along with the base case) guarantees the correct output for all problem sizes strictly less
            than $n$.) Now when $|A| = n$, the location of the maximum profit does not deviate from our
            initial assumption, i.e. it is either found on the left (in which case $L_{max}$ is the correct
            output), or on the right (in which case $R_{max}$ is the correct output) or in between, in which case
            the difference $R_{max} - L_{min}$ is the correct output. But Step $8$ of \autoref{alg:max-single-sell-profit}
            computes precisely the maximum among these values, which means for a problem of size $n$,
            we have accounted for all possible locations of the maximum profit, and picked the maximum
            one. So, if there is one, \autoref{alg:max-single-sell-profit} always outputs the correct
            maximum profit. \hfill $\square$
      \end{itemize}
    \end{enumerate}
\end{enumerate}
\end{document}
